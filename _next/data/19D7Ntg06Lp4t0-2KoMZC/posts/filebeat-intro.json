{"pageProps":{"post":{"summary":null,"tags":["Linux","大数据"],"id":"filebeat-intro","contentHtml":"<p>Filebeat日志采集工具</p>\n<h1>Filebeat介绍</h1>\n<ol>\n<li>Filebeat是一个轻量级的日志采集工具。</li>\n<li>常用的ELK日志采集方案中，大部分的做法都是将所有节点的日志内容通过filebeat送到kafka消息队列，然后使用logstash集群读取消息队列内容，根据配置文件进行过滤。最后将过滤之后的文件输送到elasticsearch中，通过kibana去展示。</li>\n<li>为什么不直接用logstash呢？因为logstash是jvm跑的，资源消耗比较大。</li>\n</ol>\n<h1>启动及配置</h1>\n<ol>\n<li>Filebeat可以配置同时输出到es和logstash以及kafka。</li>\n<li>启动Filebeat</li>\n</ol>\n<blockquote>\n<p>./filebeat -e -c filebeat.yml</p>\n</blockquote>\n<h1>采集机制</h1>\n<ol>\n<li>当开启filebeat的时候，它会启动一个或多个探测器（prospectors）去检测指定的日志目录或文件。对于探测器找出的每一个日志文件，filebeat启动收割进程（harvester），每一个收割进程读取一个日志文件的新内容，并发送这些新的日志数据到处理程序（spooler），处理程序会集合这些事件，最后filebeat会发送集合的数据到指定的地点。</li>\n</ol>\n","date":"2018-09-05T10:00:00.000Z","title":"Filebeat日志采集工具","published":true,"hideInList":false,"feature":null,"isTop":false},"prevPost":{"summary":null,"tags":["Golang"],"id":"go-defer","title":"Go defer实现","date":"2018-09-07T10:00:00.000Z","published":true,"hideInList":false,"feature":null,"isTop":false},"nextPost":{"summary":null,"tags":["HTTP"],"id":"http2-intro","title":"HTTP2介绍","date":"2018-08-08T10:00:00.000Z","published":true,"hideInList":false,"feature":null,"isTop":false}},"__N_SSG":true}